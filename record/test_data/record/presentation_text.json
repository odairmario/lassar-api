{"d2d9a672040fbde2a47a10bf6c37b6a4b5ae187f-1630953904656":{"slide-1":"Welcome To BigBlueButton\nBigBlueButton is an open source web conferencing system designed for online learning\nt\nCHAT\nSend public and private\nmessages.\nWEBCAMS\nHold visual meetings.\nAUDIO\nCommunicate using high\nquality audio.\nEMOJIS\nExpress yourself.\nPOLLING\nPoll your users anytime.\nSCREEN SHARING\nShare your screen.\nMULTI-USER WHITEBOARD\nDraw together.\nFor more information visit bigbluebutton.org ➔\n! \"\n# $\n%\n& '\nBREAKOUT ROOMS\nForm teams of users for\ngroup work.\n(\n","slide-2":"This slide left blank for whiteboard\n","slide-3":"This slide left blank for whiteboard\n","slide-4":"This slide left blank for whiteboard\n","slide-5":"This slide left blank for whiteboard\n","slide-6":"This slide left blank for whiteboard\n","slide-7":"This slide left blank for whiteboard\n","slide-8":"This slide left blank for whiteboard\n","slide-9":"This slide left blank for whiteboard\n","slide-10":"This slide left blank for whiteboard\n","slide-11":"This slide left blank for whiteboard\n","slide-12":"This slide left blank for whiteboard\n","slide-13":"This slide left blank for whiteboard\n","slide-14":"This slide left blank for whiteboard\n","slide-15":"This slide left blank for whiteboard\n"},"8698b685b5408dc170b687d58d4370771a146a52-1630954041085":{"slide-1":"ODAIR MARIO DITKUN JUNIOR\nMODELO DE ESCALONAMENTO DE TAREFAS PARA GERENCIADORES DE BANCOS\nDE DADOS EM ARQUITETURA NUMA\n(versão pré-defesa, compilada em 19 de agosto de 2021)\nTrabalho apresentado como requisito parcial à conclusão\ndo Curso de Bacharelado em Ciência da Computação,\nSetor de Ciências Exatas, da Universidade Federal do\nParaná.\nÁrea de concentração: Ciência da Computação.\nOrientador: Eduardo C. de Almeida.\nCURITIBA PR\n2021\n","slide-2":"RESUMO\nA arquitetura NUMA consiste em um modelo multiprocessadores com diferentes\nvelocidade de acesso aos bancos de memória, dado essa característica surge o efeito NUMA.\nBancos de dados relacionas em memória faz uso intensivo da memória principal do servidor e\npor isso é afetado pelo efeito NUMA. Este trabalho apresenta um algoritmo flexível que tem\ncomo objetivo minimizar o efeito NUMA nos bancos de dados.\nPalavras-chave: Palavra-chave 1. Palavra-chave 2. Palavra-chave 3.\n","slide-3":"ABSTRACT\nKeywords: Keyword 1. Keyword 2. Keyword 3.\n","slide-4":"LISTA DE FIGURAS\n1.1 Representação da arquitetura NUMA. . . . . . . . . . . . . . . . . . . . . . . . 10\n1.2 Tempo sequencial de execução de cada query . . . . . . . . . . . . . . . . . . . 11\n2.1 Representação da arquitetura NUMA. . . . . . . . . . . . . . . . . . . . . . . . 12\n2.2 Representação da rede de Petri, retirado do artigo (Dominico et al., 2018) . . . . 14\n2.3 Diagrama do modelo de execução em morsel , retirado do artigo (Leis et al., 2014) 15\n3.1 Fluxograma do escalonador. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n5.1 Tempo de médio de execução de cada query . . . . . . . . . . . . . . . . . . . . 24\n5.2 Tempo de médio de execução de cada query de todas as streams . . . . . . . . . 25\n5.3 Distribuições das threads do Monetdb com o escalonamento do sistema operacional26\n5.4 distribuições das threads do MonetDB com utilizando a estratégia gulosa e política\nde balanceamento de carga densa. . . . . . . . . . . . . . . . . . . . . . . . . . 26\n","slide-5":"LISTA DE TABELAS\n5.1 Queries por segundo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n","slide-6":"LISTA DE ACRÔNIMOS\nDINF Departamento de Informática\nPPGINF Programa de Pós-Graduação em Informática\nUFPR Universidade Federal do Paraná\nNUMA Non-uniform Memory ACCESS\nUMA Uniform Memory ACCESS\nDRAM Dynamic random-access memory\nSGDB Sistema de Gerenciadores de BAnco de Dados\n","slide-7":"LISTA DE SÍMBOLOS\nα alfa, primeira letra do alfabeto grego\nβ beta, segunda letra do alfabeto grego\nγ gama, terceira letra do alfabeto grego\nω ômega, última letra do alfabeto grego\nπ pi\nτ Tempo de resposta do sistema\nθ Ângulo de incidência do raio luminoso\n","slide-8":"SUMÁRIO\n1 INTRODUÇÃO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2 FUNDAMENTAÇÃO TEÓRICA. . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.1 ARQUITETURA NUMA. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.1.1 Efeito NUMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.1.2 MonetDB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.2 TRABALHOS RELACIONADOS . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3 PROPOSTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.1 DETALHES DE IMPLEMENTAÇÃO . . . . . . . . . . . . . . . . . . . . . . . 17\n3.1.1 Algoritmos implementados . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n3.2 ESTRATÉGIAS DE SELEÇÃO DE NODO NUMA. . . . . . . . . . . . . . . . 19\n3.3 POLíTICAS DE BALANCEAMENTO DE CARGA . . . . . . . . . . . . . . . 19\n4 EXPERIMENTOS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4.1 AMBIENTE DE EXECUÇÃO . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4.1.1 Metodologia de execução . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4.2 BENCHMARK TPC-H . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n5 RESULTADOS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n5.1 TEMPO DE EXECUÇÃO DAS CONSULTAS . . . . . . . . . . . . . . . . . . 24\n5.2 DISTRIBUIÇÃO DE TAREFAS NOS NODOS NUMAS . . . . . . . . . . . . . 25\n6 CONCLUSÃO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n6.1 TRABALHOS FUTUROS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nREFERÊNCIAS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n","slide-9":"9\n1 INTRODUÇÃO\nA arquitetura NUMA (Non Uniform Memory Access), consiste de um modelo multi-\nprocessadores, no qual cada processador possui diferentes latências de acesso aos bancos de\nmemória. A figura 2.1 representa a arquitetura NUMA,\nO sistema operacional abstrai os nodos NUMA como se fosse um único nodo, isso se\ndeve para permitir a generalização das aplicações, visto que as aplicações vão enxergar como um\núnico processador e uma memória independente da quantidade real de processadores e memória,\nessa abstração possui um custo, o núcleo do SO é responsável por distribuir as tarefas entre\nos nodos NUMAS, o escalonador scheduler realiza esse trabalho. No entanto nem sempre as\npolíticas de escalonamento beneficiam a performances das aplicações em arquitetura NUMA, isso\nse deve devido a política adotada pelo escalonador, por exemplo se a política for de distribuição\nde tarefas de forma igual entre os nodos processadores, vai ocasionar um maior acesso remoto\naos dados, gerando uma latência maior para acesso a memória, em contra partida tudo vai ficar\nbem distribuído, esse fenômeno é chamado de efeito numa (numa aware).\n","slide-10":"10\nFigura 1.1: Representação da arquitetura NUMA\nOs bancos de dados em memórias tem como características fazer cache de de dados em\nmemória para re utilizar em consultas futuras, dado essa característica a localidade da memória e\no escalonamento das tarefas se torna crucial para o desempenho do banco de dados.\nA figura 1.2 demostra os impactos do efeito NUMA no MonetDB, que é um SGDB\nrelacional, em memória e colunar. foi testados em três casos, SO é o caso que o sistema\noperacional faz o escalonamento, Denso é o caso em que as threads estão no mesmo nodo NUMA\ne só faz acesso local e por fim Esparso que as threads estão no mesmo nodo NUMA, mas a\nmemória está em outro nodo, ou seja só realiza acesso remoto. Como observado na figura forma\nque é distribuído as threads nos nodos NUMAS impacta o tempo de execução de cada query.\nA solução proposta neste trabalho consiste em um escalonador dinâmico para banco de\ndados relacionais em memória, testado no MonetDB, que avalia a similidade de endereços de\n","slide-11":"11\nFigura 1.2: Tempo sequencial de execução de cada query\nmemórias para decidir em qual nodo NUMA que cada tarefas deve ser atribuída. O algoritmo\naceita diferentes estratégia de escalonamento e de balanceamento de carga.\nO trabalho está organizado da seguinte forma: capítulo 1 introdução, Capítulo 2\nfundamentação teórica, Capítulo 3 proposta e descrição do algoritmo desenvolvido, Capítulo 4\nExperimentos executado para validação da proposta, Capítulo 5 resultados dos experimentos e\npor fim Capítulo 6 conclusão.\n","slide-12":"12\n2 FUNDAMENTAÇÃO TEÓRICA\n2.1 ARQUITETURA NUMA\nA arquitetura NUMA (Non Uniform Memory Access) é uma arquitetura de multipro-\ncessadores, em que cada processador possui sua própria memória DRAM. A memória DRAM\nDynamic Random-Access Memory de cada processador podem ser interligada entre si por uma\ninterconexão, permitindo que um processador busque dados na memória de outro processador. A\narquitetura NUMA pode ser representada pelo seguinte diagrama: 2.1\nFigura 2.1: Representação da arquitetura NUMA\nA figura 2.1 mostra uma arquitetura NUMA, no qual possui 4 processadores, cada um\ncom 3 cores e sua própria memória DRAM. O conjunto de processador e memória DRAM é\nchamado de nodo NUMA, os nodos NUMA são interligados entre si por uma interconexão, a\n","slide-13":"13\nfigura 2.1 representa a interconexão com a linha bidirecional tracejada. Quando um processador\nacessa a memória do seu noda,representado em 2.1 pela linha bidirecional continua o é chamado\nde acesso local, enquanto que o acesso de memória diferente do nodo do processador é chamado\nde acesso remoto.\nInterconexão entre os nodos NUMA possui uma latência maior que a conexão entre\nprocessador e memória local (Lameter, 2013). Essa diferença é agravada quando não existe\nconexão entre dois nodos, por exemplo na figura 2.1 não tem uma conexão direta entre os nodos\n1 e 4, para o processador acessar uma região de memória mapeada no nodo 4 é necessário passar\npela conexão do 1->2 e 2->3, se a latência de acesso local for 1ms e acesso remoto for 10ms, no\ncaso do exemplo a latência seria 20ms. Portanto a organização dos dados em memórias impacta\nno desempenho das aplicações, o melhor caso de organização é de todos os acessos a memória\nsejam local, mas isso não é possível devido a limitação de tamanho da memória e também limita\na quantidade de tarefas simultâneas por aplicação, no caso limita-se pela quantidade de cores de\ncada processador.\n2.1.1 Efeito NUMA\nO efeito NUMA é quando o processador faz acesso remoto a outros nodos NUMA,\naumentando a latência de processador->memória. Os sistemas operacionais possuem políticas\npara evitar que um espaço de memória do processo x seja alocado no nodo 1 enquanto que o\nprocesso está em execução no nodo 0 (Lameter, 2013), mas essa política, no linux, possui alguns\nentraves como o algoritmo de balanceamento de carga e quando o processo possui mais threads\ndo que cores disponíveis em um único nodo NUMA da máquina.\nO kernel linux, possui desde a versão 3.8 (Lameter, 2013), mecanismo para lidar com\narquitetura numa de forma eficiente, como por exemplo ele sempre tenta alocar memória no\nmesmo nodo numa que o processo está em execução. Mas quando o processo possui mais threads\nativas do que a quantidade de cores do nodo numa, essas threads serão distribuídas entre os\noutros nodos NUMA e a memória do processo permanece no nodo 1, enquanto que as threads\nestão espalhadas entres todos os nodos, o que implica que as threads irão fazer acesso remoto e o\ndesempenho da aplicação vai ser reduzido.\nNos bancos de dados o efeito NUMA é agravado, devido a natureza da aplicação. Como\nvisto no gráfico: , o desempenho do SGDB MonetDB é superior quando todas as suas threads\nestão no mesmo nodo, como demonstrado na figura 1.2.\n2.1.2 MonetDB\nO MonetDB é um gerenciador de banco de dados relacional baseado em memória com\narmazenamento colunar, ou seja a organização interna do armazenamento é feito por coluna e\nnão por linha. O MonetDB foi projetado para aplicações de data warehouse e OLAP (Idreos\net al., 2012).\n","slide-14":"14\nO MonetDB armazena cada coluna em um arquivo separado, do tipo bat, cada arquivo\npossui dois valores por linha, o primeiro é o UID da coluna e o segundo é o valor da coluna (Idreos\net al., 2012). Essa organização permite o MonetDB carregue grande parte de uma determinada\ncoluna em memória contígua, que favorece o uso de cache l2 e l1, tornando eficiente para\noperações sobre coluna como média ou soma. No entanto operações como Update se torna um\npouco custoso pois o SGDB precisará carregar múltiplas colunas para poder fazer a atualização.\nAs tabelas do MonetDB são persistidas em disco e conforme utilizadas é mantida\nem memória, tornando o acesso mais rápido após o segundo uso. As consultas podem ser\nparalelizadas no nível intra-query com uso de threads. O efeito NUMA é significativo no\nMonetDB devido ao seu uso de memória e threads.\n2.2 TRABALHOS RELACIONADOS\nO artigo (Dominico et al., 2018) apresenta uma solução para NUMA aware em bancos\nde dados relacional OLAP, baseado em rede de PetriNet, no qual a utilização de recursos e decide\nem que nodo NUMA será alocado. A figura 2.2 elaborado pelos autores do artigo (Dominico\net al., 2018) exemplifica o funcionamento da rede de Petri.\nFigura 2.2: Representação da rede de Petri, retirado do artigo (Dominico et al., 2018)\n","slide-15":"15\nO artigo (Leis et al., 2014) apresenta um modelo de execução paralelo de granularidade\nfina, no qual os dados são divididos em pequenos fragmentos, chamado de morsels e cada\nfragmentado é relacionado à uma threads que vai executar em determinado nodo NUMA. A\nfigura 2.3, elaborado pelos autores do artigo (Leis et al., 2014) exemplifica o modelo de execução\nem morsel, no qual o dispatcher separa os dados em morsel e atribuí para uma threads em\ndeterminado nodo NUMA, após a execução os fragmentos são reunidos novamente para formar a\nsaída.\nFigura 2.3: Diagrama do modelo de execução em morsel , retirado do artigo (Leis et al., 2014)\nO artigo (Srinivasa e Sosonkina, 2012) apresenta uma solução, no qual identifica blocos\nde memória com afinidade a determinada thread e fixa o bloco no banco de memória do nodo\nNUMA da thread com afinidade. A identificação dos blocos de memória com afinidade as\nthreads é feita com base na classificação dos acessos determinismo e não determinismo.\n","slide-16":"16\n3 PROPOSTA\nA proposta apresentada neste trabalho consiste em um algoritmo genérico capaz de\nreceber um conjunto de tarefas do sistema operacional e determinar a melhor distribuição das\ntarefas nos nodos NUMAS. O algoritmo tem como princípios ser genérico, flexível e adaptável.\nE por isso é dividido em etapas e cada etapa permite modificações, como alteração de políticas de\nescalonamento, múltiplas formas de coleta informações das tarefas ativas e também o método de\natribuir as tarefas no nodo. O algoritmo foi dividido em 3 etapas, para auxiliar na compreensão\ndo seu funcionamento, são elas:\nEtapa 1: Coleta de dados A primeira etapa do algoritmo consiste em um método de coletar\nas tarefas ativas do banco de dados, no diagrama 3.1 é representado pelo número 2. A saída\ndessa etapa deve ser uma lista de identificação de cada tarefa, no caso do linux o PID.\nEtapa 2: Escolha do nodo preferido A segunda etapa consiste em comparar os endereços de\nmemória de cada tarefa coletada na etapa anterior 3 com os bancos de tarefas já escalonado, e\ncom base na comparação escolher um nodo como preferido, a forma de escolha é feita com base\nem uma estratégia definida no tempo de inicialização, são apresentadas algumas estratégia na\nseção 3.2. A saída deve ser o identificador de um nodo NUMA ou vazio, caso nenhum nodo seja\no preferido.\nEtapa 3: Atribuição de um nodo NUMA à uma tarefa A terceira etapa recebe um nodo\npreferido pela etapa anterior3 e verifica se esse nodo está disponível para receber mais tarefas, se\nsim aloca então a tarefa no nodo caso contrário escolhe um nodo com base em alguma política\nde balanceamento de carga. É importante verificar a disponibilidade do nodo NUMA preferido,\npois caso contrário acontece um fenômeno interessante de todas as tarefas ficarem no mesmo\nnodo NUMA e os outros ficam ociosos.\nDiagrama de execução O diagrama de execução 3.1 apresenta de forma visual o funcionamento\ndo algoritmo, podemos classificar cada bloco em umas das etapas apresentada anteriormente. Os\nblocos 1,2,3 fazem parte da etapa 1 3, blocos 5,4,6 pertence à etapa 2 3 e os blocos 7,8,9,10 a\netapa 3 3.\n","slide-17":"17\nFigura 3.1: Fluxograma do escalonador\n3.1 DETALHES DE IMPLEMENTAÇÃO\nCalculo da semelhança Foi utilizado o índice de similaridade de jaccard 3.1 para determinar\na similaridade entre duas tarefas.\nSeja ti e tj duas tarefas, a fórmula de jaccard pode ser definida como:\nSi,j =\n|M(ti) ∩ M(tj)|\n|M(ti) ∪ M(tj)|\n(3.1)\nOnde:\nSi,j : é a semelhança da tarefa ti e tarefa tj .\nM : é conjunto de endereços de memória e M(ti é o conjunto de memória da tarefa ti\n3.1.1 Algoritmos implementados\nOs pontos mais relevantes da implementação serão apresentados e discutidos, como a\nfunção principal e a função de adição de tarefas, a função de remoção de tarefas da estrutura de\ndados do escalonado não será discutido neste trabalho, por ser algo trivial, consiste apenas em\nremover da fila de prioridade.\n","slide-18":"18\nFunção principal O algoritmo 1 é responsável por verificar se existes novas tarefas, se sim\nentão adiciona, também é responsável por remover tarefas que não estão mais ativas. A função\npermanece no laço até que receba um sinal de término do sistema operacional.\nAlgoritmo 1 Função principal do escalonador\ntare f as_escalonadas ← ∅\nescalonador_ativo ← True\nwhile escalonador_ativo do\ntare f as_atuais ← consulta_tare f as()\nnovas_tare f as ← tare f as_atuais \\ tare f as_escalonadas\ntare f as_encerradas ← tare f as_escalonadas \\ tare f as_atuais\nadiciona_tare f as(novas_tare f as)\nremove_tare f as(tare f as_encerradas)\nend while\nFunção de adição de tarefas A função de adição de tarefas é o ponto central do escalonador,\né nela que se calcula a similaridade das novas tarefas com as tarefas existentes, escolhe um\nnodo como preferido, verifica se o nodo está disponível e por fim aloca a tarefa em um nodo da\narquitetura NUMA.\nAlgoritmo 2 Função de adição de tarefas\nfor each ti ∈ T\nnovas do\nSi ← ∅\nfor each tj ∈ T\natuais do\nsi,j ←\n|M(ti)∩M(tj)|\n|M(ti)∪M(tj)|\nSi ← Si ∪ {si,j }\nend for\nnodo_pre f erido ← escolhe_nodo(Si)\nif veri f ica_nodo(nodo_pre f erido) then\nnodo ← nodo_pre f erido\nelse\nnodo ← load_balance()\nend if\naloca_nodo(ti,nodo)\nTatuais ← Tatuais ∪ {ti}\nend for\nEstruturas de dados utilizados no escalonado São utilizados algumas estruturas de dados no\nescalonador para armazenar informações do escalonador e das tarefas já tratadas e novas tarefas.\n","slide-19":"19\nAs maiorias das estruturas utilizadas são conjuntos de dados, a estrutura que mais difere é o da\nvariável Si do algoritmo 2, que é uma fila de prioridade, implementado como uma heap.\n3.2 ESTRATÉGIAS DE SELEÇÃO DE NODO NUMA\nA segunda parte do algoritmo 3 define que é necessário uma estratégia para escolher\nqual nodo NUMA é o mais adequado para a tarefa. A estratégia recebe como parâmetro uma\ntarefa t e o conjunto de tarefas já escalonada pelo algoritmo T e deve devolver um nodo preferido,\ndessa forma é possível implementar múltiplas estratégia, que permite maior flexibilidade do\nalgoritmo.\nForam elaborados duas estratégia, a primeira com abordagem gulosa e a segunda\nseleciona o nodo no qual possui mais tarefas semelhantes\nEstratégia gulosa A estratégia gulosa consiste em escolher o nodo, no qual reside a tarefa com\nmaior similaridade com a tarefa a ser adicionada. É possível fórmula a estratégia como:\nnodo = Nodos(max\ntj ∈T\nS(tj,ti)) (3.2)\nOnde\nS(tj,ti) é o calculo da semelhança da tarefa tj e ti.\nNodos(t) é o nodo que está alocado a tarefa t, no caso a tarefa com maior semelhança.\nEstratégia do nodo correspondente A estrategia de nodo correspondente consiste em escolher\no nodo no qual reside a maior quantidade de tarefas que a semelhança com a tarefa ser alocado\nseja maior ou igual a um limite definido na inicialização do programa.\n3.3 POLÍTICAS DE BALANCEAMENTO DE CARGA\nA escolha de um nodo preferido com base em umas das estrategias definida em 3.2\nnão garante que a tarefa será alocada no nodo escolhido, isso se deve devido ao fato de que a\nestratégia pode alocar todas as tarefas para o mesmo nodos, sobrecarregando um nodo enquanto\nos outros nodos estão ociosos. Portanto é necessário se ter um balanceamento de carga para evitar\nsobrecarga do nodo NUMA, que pode gerar uma performance inferior do que com NUMA aware.\npolítica de balanceamento de carga Denso A política de balanceamento de carga Denso\nconsiste em preencher os nodos NUMAS com tarefas antes de partir para o próximo nodo. A\nideia desse balanceamento é aglutinar o maior número de tarefas no menor número de NUMA\nnodo sem extrapolar a quantidade de núcleos de cada nodo. E portanto reduzir o acesso remoto,\nmas esse modelo possui os seus problemas, visto que não surte efeito quando o banco de dados\nutilizar todos os núcleo do servidor.\n","slide-20":"20\npolítica de balanceamento de carga Esparso A política esparsa é o oposto da política\ndensa 3.3, ou seja tem como objetivo espalhar as tarefas entre todos os nodos NUMAS, de forma\na reduzir de forma geral o acesso remoto, quando o banco de dados utiliza todos os núcleos do\nservidor, a melhor forma de minimizar o efeito NUMA é distribuir a memória entre todos os\nnodos.\npolítica de balanceamento de carga Aleatório A política aleatório consiste em escolher de\nforma aleatória um nodo NUMA.\n","slide-21":"21\n4 EXPERIMENTOS\n4.1 AMBIENTE DE EXECUÇÃO\nO ambiente de execução utilizado para validação da proposta possui as seguintes\ncaracterísticas:\nMemória : 132GiB\nProcessador : Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz\nLoad médio antes do experimento : 0.0\nQuantidade de cores : 16\nThreads por core : 1\nFrequência de relógio : 3.2 GHz\nNodos NUMA : 2\nMemória por nodo NUMA : 64GiB\nCores por nodo NUMA : 8\nO load médio representa que antes da execução não houve nenhuma atividade na servidora que\npossa interferir nos experimentos.\n4.1.1 Metodologia de execução\nFoi utilizado o benchmark TPC-H para validar a proposta 3. Os testes seguiram a\nespecificação do TPC-H (Poess e Nambiar, 2010), foi utilizado diferentes tamanhos de banco\nde dados e em alguns casos executados os testes n vezes e feito a média aritmética e desvios\npadrão, em outros casos foi executado apenas uma vez, visto que não foi observado alterações\nnos resultados.\nMétricas As métricas definida na especificação do TPC-H (Poess e Nambiar, 2010), tempo de\nexecução de cada consulta e a distribuição das tarefas nos nodos NUMA. A métrica do TPC-H na\nespecificação é definido como a quantidade de consultas feita por hora pelo gerenciador de banco\nde dados, mas como foi trabalho com escala de banco de dados menores, foi utilizado a métrica\nquantidade de consultas por segundo, apenas uma divisão por 36000 e ajuda a mensurar se\nhouve melhora na performance do banco. A métrica de tempo serve para entender em quais\nconsultas houve melhora de desempenho e quais consultas houve piora de desempenho piorou.\n","slide-22":"22\n4.2 BENCHMARK TPC-H\nO TPCH-H é um benchmark de apoio de decisão, no qual consiste em um conjunto de\nconsultas OLAP(Poess e Nambiar, 2010). O benchmark é constituído de uma base de dados com\ndiferentes escalas e de um conjunto de 22 consultas que visam avaliar o desempenho do banco de\ndados para consultas OLAP. As métricas são geradas a partir de dois testes do benchmark, o\npower test e throughput test.\nPower test O power test consiste na execução sequencial das 22 consultas do benchmark, em\ndeterminada ordem definida na especificação do TPC-H (Poess e Nambiar, 2010). Os resultados\ndo power test são utilizado pelo TPC-H para computar a capacidade de processamento de\nconsultas únicas, os resultados são agregados em uma fórmula que avalia a capacidade do banco\nde dados definido em:\nTPC − HPower@Size =\n3600 ∗ SF\n24\nqÎ22\ni=1 QI(i,0) ∗\nÎ2\nj=1 RI(j,0)\n(4.1)\nOnde:\nQI(i,0) : é o tempo de execução, em segundos, da query i, e 0 é a ordem de execução, definido\nno apêndice da especificação do tpch (Poess e Nambiar, 2010).\nRI(j,0) : É a função de atualização de dados, no qual j é o número da função e 0 a ordem de\nexecução.\nSize : É o tamanho da base de dados definido na especificação do TPC-H\nThroughput test O Throughput test consiste na execução paralela das consultas, o objetivo\né avaliar a capacidade do banco de dados de lidar com múltiplas transações simultâneas . A\nmétrica gerada por esse teste é definido pela equação:\nTPC − HThroughput@Size =\nS ∗ 22 ∗ 3600\nTs\n∗ SiZE (4.2)\nOnde:\nS É a quantidade de execuções paralelas, referida como stream\nTs é o tempo total de execução de todas stream\nSize : É o tamanho da base de dados definido na especificação do TPC-H\n","slide-23":"23\n5 RESULTADOS\nForam executados os experimentos com os seguintes algoritmos:\nSO : escalonador do sistema operacional, serve como base de comparação\nDenseGreedyAlgorithm : Escalonador proposto utilizando a estratégia gulosa 3.2 e política de\nbalanceamento densa ??.\nDenseGeneralAlgorithm : Escalonador proposto utilizando a estratégia do nodo correspon-\ndente 3.2 e política de balanceamento densa ??.\nRandomGreedyAlgorithm : Escalonador proposto utilizando a estratégia gulosa 3.2 e política\nde balanceamento densa ??.\nRandomGeneralAlgorithm : Escalonador proposto utilizando a estratégia do nodo correspon-\ndente 3.2 e política de balanceamento densa ??.\nSparseGreedyAlgorithm : Escalonador proposto utilizando a estratégia gulosa 3.2 e política\nde balanceamento densa ??.\nSparseGeneralAlgorithm : Escalonador proposto utilizando a estratégia do nodo correspon-\ndente 3.2 e política de balanceamento densa ??.\nTabela de desempenho TPC-H A tabela 5.1 apresenta os desempenhos dos algoritmos nas\nmétricas definido pela especificação do TPC-H.\n","slide-24":"24\nAlgoritmo Teste TPC-H Queries por segundos\nDenseGeneralAlgorithm throughput 4.562\npower 2.369\nDenseGreedyAlgorithm throughput 5.232\npower 4.137\nRandomGeneralAlgorithm throughput 4.758\npower 3.989\nRandomGreedyAlgorithm throughput 4.617\npower 4.022\nSistema operacional throughput 4.532\npower 4.075\nSparseGeneralAlgorithm throughput 4.911\npower 4.112\nSparseGreedyAlgorithm throughput 4.855\npower 3.968\nTabela 5.1: Queries por segundo\n5.1 TEMPO DE EXECUÇÃO DAS CONSULTAS\nO tempo de execução das consultas não é a métrica mais adequada, quando se trata do\nbenchmark TPC-H, visto que possui métricas próprias (Poess e Nambiar, 2010), mas é relevante\npara nossos objetivo, visto que é possível identificar em quais consultas houve melhora ou piora.\nTempo de execução das consultas do power test A figura 5.1 mostra o tempo de execução\nde cada query do benchmark TPC-H, o algoritmo DenseGreedyAlgorithm se saiu melhor em\nquase todas queries. É importante observar que houve alguns outline na execução do sistema\noperacional, principalmente na query 9, 13, 14, 18 e 21, o heatmap do sistema operacional 5.3\npode ajudar a entender o que aconteceu com essas queries.\nFigura 5.1: Tempo de médio de execução de cada query\nTempo de execução das consultas do throughput test A figura 5.2 evidencia um desvio\npadrão alto em todas as consultas, isso se deve a concorrência de de cache do processador\n","slide-25":"25\npela múltiplas streams. Dado o resultado do gráfico não é possível concluir se houve melhora\nsignificativa no tempo das queries devido ao alto desvio padrão, para avaliar a capacidade de\nprocessamento paralelo deve ser utilizado a métrica definida pela especificação do TPC-H (Poess\ne Nambiar, 2010).\nFigura 5.2: Tempo de médio de execução de cada query de todas as streams\n5.2 DISTRIBUIÇÃO DE TAREFAS NOS NODOS NUMAS\nO ponto central do escalonamento é a distribuição das tarefas nos nodos NUMAS, então\né importante observar gráficos que demonstram a distribuição das threads, foi utilizado o modelo\nde gráfico heatmap para representar a distribuição de tarefas nos nodos NUMAS, o tempo que foi\nobservado, em segundos, o eixo Y representa os PIDS da threads e o z o id do nodo NUMA, no\ncaso 0 ou 1. O framework utilizado para geração dos gráficos do presente trabalho não permite\nheatmap com cores sequências, portanto todos os próximos heatmaps estão com a barra de cor\nem escala contínua, então deve ser compreendido como escala sequencial.\nFoi escolhido a query 9 para observar a distribuição das tarefas nos nodos NUMA, por\ncausa do comportamento fora da curva em 5.1. Foram selecionados somente o heatmap do\nsistema operacional e o do algoritmo DenseGreedyAlgorithm que foi o que se saiu melhor, as\noutras variações do algoritmo obteve resultados semelhantes então para não poluir o resultado\nfoi deixado apenas o melhor resultado.\nHeatmap Sistema operacional O heatmap do sistema operacional 5.3 mostra que o escalonador\ndo sistema operacional não fixa as tarefas em um nodo NUMA, migrando as tarefas entre os nodos\nNUMA a cada intervalo de tempo. O comportamento do escalonador do sistema operacional\njustifica o pérsimo desemepnho da consulta 9 no gráfico 5.1\n","slide-26":"26\nFigura 5.3: Distribuições das threads do Monetdb com o escalonamento do sistema operacional\nHeatmap DenseGreedyAlgorithm O heatmap da figura 5.4 mostra a distribuição das tarefas\nque o algoritmo faz com a estratégia gulosa e política de balanceamento de carga denso, como\npode ser observado, as threads não são migradas ao longo da execução e existe uma divisão entre\nos nodos NUMA.\nFigura 5.4: distribuições das threads do MonetDB com utilizando a estratégia gulosa e política de balanceamento de\ncarga densa\n","slide-27":"27\n6 CONCLUSÃO\nO efeito NUMA acontece quando o servidor em arquitetura NUMA não escalona as\ntarefas levando em conta os nodos NUMA, gerando aumento de acesso à memória remota. O\npresente trabalho apresenta uma proposta que tem como objetivo minimizar o efeito NUMA\nem bancos de dados relacionais em memória, foi utilizado o banco de dados MonetDB para os\nexperimentos.\nA proposta apresentada consiste de um algoritmo genérico e flexível que escalona as\ntarefas do banco de dados com base no cálculo da semelhança dos endereços de memória das\nthreads existentes. Os resultados foram positivos, apresentando melhora significativa melhora\nno desempenho do benchmark TPC-H. Foi observado que dependendo do tipo de execução a\npolítica de balanceamento de carga influencia diretamente no desempenho, como demostrado\nem 5.1 que a política densa apresentou uma piora nos testes sequências enquanto que nos testes\nparalelos foi o melhor.\n6.1 TRABALHOS FUTUROS\nO algoritmo proposto não permite mudança em tempo de execução das estratégia de\nescolha de nodo NUMA e da política de balanceamento, então uma possível sugestão de trabalhos\nfuturos seria tornar o algoritmo adaptativo que conforme a característica de execução do banco\nde dados muda a política de balanceamento de denso para esparso. Por exemplo, se o algoritmo\nidentificar que o banco de dados está executando apenas consultas sequências utilizar a política\ndensa enquanto que se estiver executando múltiplas consultas alternar para política esparso.\nOs testes foram feitos somente no banco de dados MonetDB, dado sua característica de\nser em memória, fica como sugestão testar e avaliar o desempenho dos algoritmos em banco de\ndados não colunar ou que não faz uso extensivo da memória principal.\n","slide-28":"28\nREFERÊNCIAS\nDominico, S., de Almeida, E. C., Meira, J. A. e Alves, M. A. (2018). An elastic multi-core\nallocation mechanism for database systems. Em 2018 IEEE 34th International Conference on\nData Engineering (ICDE), páginas 473–484.\nIdreos, S., Groffen, F., Nes, N., Manegold, S., Mullender, S. e Kersten, M. (2012). Monetdb:\nTwo decades of research in column-oriented database architectures. IEEE Data Eng. Bull., 35.\nLameter, C. (2013). Numa (non-uniform memory access): An overview: Numa becomes more\ncommon because memory controllers get close to execution units on microprocessors. Queue,\n11(7):40–51.\nLeis, V., Boncz, P., Kemper, A. e Neumann, T. (2014). Morsel-driven parallelism: A numa-aware\nquery evaluation framework for the many-core age. Em Proceedings of the 2014 ACM SIGMOD\nInternational Conference on Management of Data, SIGMOD ’14, página 743–754, New York,\nNY, USA. Association for Computing Machinery.\nPoess, M. e Nambiar, R. (2010). Tpc benchmark h standard specification.\nSrinivasa, A. e Sosonkina, M. (2012). Nonuniform memory affinity strategy in multithreaded\nsparse matrix computations. Em Proceedings of the 2012 Symposium on High Performance\nComputing, HPC ’12, San Diego, CA, USA. Society for Computer Simulation International.\n"}}
